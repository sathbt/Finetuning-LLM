{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathbt/Finetuning-LLM/blob/main/llamaindex/Combining_Text_to_SQL_with_Semantic_Search_for_Retrieval_Augmented_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining Text-to-SQL with Semantic Search for `RAG` [LlamaIndex Website](https://www.llamaindex.ai/)"
      ],
      "metadata": {
        "id": "D4GclYQPYmV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we show you how to use our `SQLAutoVectorQueryEngine`. More info in this [blog post](https://blog.llamaindex.ai/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b).\n",
        "\n",
        "- This query engine allows you to combine insights from your structured tables with your unstructured data.\n",
        "- Can leverage both a SQL database as well as a vector store to fulfill complex natural language queries over a combination of structured and unstructured data\n",
        "- It first decides whether to query your structured tables for insights. Once it does, it can then infer a corresponding query to the vector store in order to fetch corresponding documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "AqXB6DiGRvzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ],
      "metadata": {
        "id": "42Y7G6NYSNs5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9rm4RRQEQiTS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index openai pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# find API key in console at https://platform.openai.com/account/api-keys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "1j_K_9xxQqa4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        "    SQLDatabase,\n",
        "    WikipediaReader,\n",
        ")"
      ],
      "metadata": {
        "id": "vaZ90v_nSCFg",
        "outputId": "4dcdfa25-42fe-4215-91c6-0dde9762a351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'VectorStoreIndex' from 'llama_index' (unknown location)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-68a9ed8a4b3d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from llama_index import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSimpleDirectoryReader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mServiceContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mStorageContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'VectorStoreIndex' from 'llama_index' (unknown location)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Common Objects\n",
        "This includes a `ServiceContext` object containing abstractions such as the LLM and chunk size. This also includes a `StorageContext` object containing our vector store abstractions."
      ],
      "metadata": {
        "id": "N2Ctq_RwXnD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define pinecone index\n",
        "import pinecone\n",
        "import os\n",
        "\n",
        "# find API key in console at https://app.pinecone.io/\n",
        "os.environ['PINECONE_API_KEY'] = 'PINECONE_API_KEY'\n",
        "# environment is found next to API key in the console\n",
        "os.environ['PINECONE_ENVIRONMENT'] = 'asia-southeast1-gcp'\n",
        "\n",
        "# initialize connection to pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.environ['PINECONE_API_KEY'],\n",
        "    environment=os.environ['PINECONE_ENVIRONMENT']\n",
        ")\n",
        "\n",
        "# dimensions are for text-embedding-ada-002\n",
        "pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")"
      ],
      "metadata": {
        "id": "oL8gqFvYSG5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list indexes\n",
        "pinecone.list_indexes()"
      ],
      "metadata": {
        "id": "dWL-cT7le2Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe index\n",
        "pinecone.describe_index(\"quickstart\")"
      ],
      "metadata": {
        "id": "SzRkpcLje_cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to the index\n",
        "pinecone_index = pinecone.Index('quickstart')"
      ],
      "metadata": {
        "id": "M4TNC820fVkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser.simple import SimpleNodeParser\n",
        "from llama_index import ServiceContext, LLMPredictor\n",
        "from llama_index.storage import StorageContext\n",
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "from llama_index.text_splitter import TokenTextSplitter\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "# define node parser and LLM\n",
        "chunk_size = 1024\n",
        "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", streaming=True)\n",
        "service_context = ServiceContext.from_defaults(chunk_size=chunk_size, llm=llm)\n",
        "text_splitter = TokenTextSplitter(chunk_size=chunk_size)\n",
        "node_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter)\n",
        "\n",
        "# define pinecone vector index\n",
        "vector_store = PineconeVectorStore(\n",
        "    pinecone_index=pinecone_index, namespace=\"wiki_cities\"\n",
        ")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "vector_index = VectorStoreIndex([], storage_context=storage_context)"
      ],
      "metadata": {
        "id": "_IIcuNH5ZWcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Database Schema + Test Data"
      ],
      "metadata": {
        "id": "oSMgJ0g4ZcOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        "    select,\n",
        "    column,\n",
        ")"
      ],
      "metadata": {
        "id": "gNqlwqBbZqd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = create_engine(\"sqlite:///:memory:\", future=True)\n",
        "metadata_obj = MetaData()"
      ],
      "metadata": {
        "id": "nyZENLhzZsfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create city SQL table\n",
        "table_name = \"city_stats\"\n",
        "city_stats_table = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"city_name\", String(16), primary_key=True),\n",
        "    Column(\"population\", Integer),\n",
        "    Column(\"country\", String(16), nullable=False),\n",
        ")\n",
        "\n",
        "metadata_obj.create_all(engine)"
      ],
      "metadata": {
        "id": "ijFHTzx2Z5Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print tables\n",
        "metadata_obj.tables.keys()"
      ],
      "metadata": {
        "id": "YgypApzTZ94x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We introduce some test data into the `city_stats` table"
      ],
      "metadata": {
        "id": "YRsiYD6NaGOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import insert\n",
        "\n",
        "rows = [\n",
        "    {\"city_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n",
        "    {\"city_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n",
        "    {\"city_name\": \"Berlin\", \"population\": 3645000, \"country\": \"Germany\"},\n",
        "]\n",
        "for row in rows:\n",
        "    stmt = insert(city_stats_table).values(**row)\n",
        "    with engine.connect() as connection:\n",
        "        cursor = connection.execute(stmt)\n",
        "        connection.commit()"
      ],
      "metadata": {
        "id": "Xivl55K0aHEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with engine.connect() as connection:\n",
        "    cursor = connection.exec_driver_sql(\"SELECT * FROM city_stats\")\n",
        "    print(cursor.fetchall())"
      ],
      "metadata": {
        "id": "Q9KQHGZKaJ8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "- Lets use the [Wikipedia Loader](https://llamahub.ai/l/wikipedia) from LlamaHub."
      ],
      "metadata": {
        "id": "D0MlCMElaWx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "IdvKhH3Cf0Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "\n",
        "loader = WikipediaReader()\n",
        "cities = [\"Toronto\", \"Berlin\", \"Tokyo\"]\n",
        "wiki_docs = loader.load_data(pages=cities)"
      ],
      "metadata": {
        "id": "9QolASaXaYi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wiki_docs)"
      ],
      "metadata": {
        "id": "WlzMBem-dfaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build SQL Index"
      ],
      "metadata": {
        "id": "iLGpCMRnYNjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])"
      ],
      "metadata": {
        "id": "6TZThBbmXtIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine"
      ],
      "metadata": {
        "id": "Z0E3Kvq8YaQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql_query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=sql_database,\n",
        "    tables=[\"city_stats\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Mh2NgVY6Y38e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Vector Index"
      ],
      "metadata": {
        "id": "aaZg6hK4Y9zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert documents into vector index\n",
        "# Each document has metadata of the city attached\n",
        "for city, wiki_doc in zip(cities, wiki_docs):\n",
        "    nodes = node_parser.get_nodes_from_documents([wiki_doc])\n",
        "    # add metadata to each node\n",
        "    for node in nodes:\n",
        "        node.metadata = {\"title\": city}\n",
        "    vector_index.insert_nodes(nodes)"
      ],
      "metadata": {
        "id": "s47gJhECZAB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Query Engines, Set as Tools"
      ],
      "metadata": {
        "id": "zntFQ9bIbkir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine import SQLAutoVectorQueryEngine, RetrieverQueryEngine\n",
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "from llama_index.indices.vector_store import VectorIndexAutoRetriever\n",
        "from llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo"
      ],
      "metadata": {
        "id": "f9n9g7DRblce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store_info = VectorStoreInfo(\n",
        "    content_info=\"articles about different cities\",\n",
        "    metadata_info=[\n",
        "        MetadataInfo(name=\"title\", type=\"str\", description=\"The name of the city\"),\n",
        "    ],\n",
        ")\n",
        "vector_auto_retriever = VectorIndexAutoRetriever(\n",
        "    vector_index, vector_store_info=vector_store_info\n",
        ")\n",
        "\n",
        "retriever_query_engine = RetrieverQueryEngine.from_args(\n",
        "    vector_auto_retriever, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "s5hPAZn8b2yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=sql_query_engine,\n",
        "    description=(\n",
        "        \"Useful for translating a natural language query into a SQL query over a table containing: \"\n",
        "        \"city_stats, containing the population/country of each city\"\n",
        "    ),\n",
        ")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=retriever_query_engine,\n",
        "    description=f\"Useful for answering semantic questions about different cities\",\n",
        ")"
      ],
      "metadata": {
        "id": "7Dy7PsbXcK_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define `SQLAutoVectorQueryEngine`"
      ],
      "metadata": {
        "id": "H1V5bGZ8ccI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = SQLAutoVectorQueryEngine(\n",
        "    sql_tool, vector_tool, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "E3pfh93Cce--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query\n",
        "**The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer.**"
      ],
      "metadata": {
        "id": "TEbZV1tBc9MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Can you give me the country corresponding to each city?\")\n",
        "response.response"
      ],
      "metadata": {
        "id": "G5WsUm6LdLQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.metadata"
      ],
      "metadata": {
        "id": "jr0AdrVKmGcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Tell me about the history of Berlin\")\n",
        "response.response"
      ],
      "metadata": {
        "id": "pNmkQl_UdG52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.metadata"
      ],
      "metadata": {
        "id": "VW5d3wd9mLXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Tell me about the arts and culture of the city with the highest population\"\n",
        ")"
      ],
      "metadata": {
        "id": "PjgDScSBsbje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "id": "1ow2PHYysfdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-4 is needed for Querying both tool"
      ],
      "metadata": {
        "id": "IuZyYHDZlw06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.7, model=\"gpt-4\", streaming=True)\n",
        "service_context = ServiceContext.from_defaults(chunk_size=chunk_size, llm=llm)\n",
        "\n",
        "query_engine = SQLAutoVectorQueryEngine(\n",
        "    sql_tool, vector_tool, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "BfT2KL72iUon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Tell me about the arts and culture of the city with the highest population\"\n",
        ")"
      ],
      "metadata": {
        "id": "CL6z0zTAlbY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.response"
      ],
      "metadata": {
        "id": "mOU1AbellebI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.metadata"
      ],
      "metadata": {
        "id": "iW744K9Yl7Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the pinecone index\n",
        "pinecone.delete_index(\"quickstart\")"
      ],
      "metadata": {
        "id": "m0yxr-Bbmzbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "- Chunking plays a vital role.\n",
        "- Need to play around with different models to get the right answer.\n",
        "- Trial and error is what it is needed for LLMs."
      ],
      "metadata": {
        "id": "m-H1ncQmmRm6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyGBQ5uFl9MJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}